==클라우드 컴퓨팅 : '빌려쓰기'
서비스 제공을 위해 필요한 물리적인프라( 서버, 네트워크장비, 방화벽,)
개발환경(프로그래밍언어 ,DB, web), 애플리케이션(포토샵, 웹서비스)
클라우드라는 공간에 배치해두고 사용자는 이를 필요한 만큼 빌려서 사용하는 것.

사용자입장에서 자신이 원하는 환경을 직접구축하는 것이 아니라
빌려서 사용하는 개념

클라우드 컴퓨팅은 반드시 가상화를 동반해야하는 것은아니나
가상화를 도입하는 경우 공급자입장에서 운영이 수월해진다
또한 추가적인 장점( ex)무중단서비스, 지역간 이동이 가능하다[live migration])이 있음


==클라우드 컴퓨팅의 분류
-SaaS (소프트웨어 자체를 제공 - saleforce.com)
-PaaS (개발 환경을 제공 -docker, kubernetes )
-IaaS(컴퓨팅자원 제공- CPU, RAM,HDD)
  > openctak, KVM, VMware vSphere, aws ec2/rds, gcp

*aws 의 rds는 실제로는 서버가 하나 만들어진뒤 그위에 DB를 생성하여 제공하는 형태임
그러므로 엄밀히말하면 IaaS로 볼수있음

==오픈스택 재단이 주도하여 6개월마다 업데이트를 릴리즈

--오픈스택은 프라이빗 클라우드임

==오픈스택의 대표적인 서비스
1.keystone : (인증서비스, 데이터베이스-MariaDB) 
아래의 모든 서비스에 접근하기 위해서는 keystone을 통해서 접근해야함

2.glance :

3.nova : (computing서비스 , 가상머신(인스턴스) 생성, 스토리지 생성 > Hypervisor )
(default hypervisor : KVM , 상용버전인 VMware ESXi를 사용할수도 있음)
무언가 새로운것을 만들어내는것은 nova서비스

4.horizon

5.Neutron

6.Cinder 

7.Swift (Storage 서비스, 각 사용자별로 일정공간을 제공하는 서비스,
object storage : 로그인에따라서 사용자별로 독립적인 사용공간[container]을 
제공함)

==오픈스택의 기본구조 - 대분류
1.compute ex) Nova
2.network  ex) Neutron
3.storage   ex) cinder,swift

서비스의 연결은 API를 통해서 가능
Dash Board서비스 

==오픈스택의 버전
Austin : nova, swift


--오픈스택 환경구축
vmnet1 
---------------------------------------------------------
  |192.168.1.100/24      |.101		    |.102		
control		
network		compute1		compute2
storager		   CPU4		cpu4,ram4
[cpu4 ram8	    ram4		disk 120
disk 240] 		DISK 120			
  | 10.5.116.100/8 	   |.101 		   | .102
---------------------------------------------------------
bridge

bridge 대역 : 10.5.116.x/8대역을 사용하기

3노드 모두 

firwalld/selinux/NetworkManager disable stop
yum -y update

/etc/hosts에 추가
control		192.168.1.100
compute1 	192.168.1.101
compute2	192.168.1.102
>echo -e "control\t192.168.1.100\ncompute1\t192.168.1.101\ncompute2\t192.168.1.102" /etc/hosts 

vi /etc/default/grub 에서 6번째 행의 마지막에 net.ifnames=0 biosdevname=0 입력


      6 rhgb quiet net.ifnames=0 biosdevname=0"

> sed -i "s/quiet/quiet net.ifnames=0 biosdevname=0/g" /etc/default/grub

sed -i "s/quiet/quiet net.ifnames=0 biosdevname=0/g" /etc/defualt/grub
mv /etc/sysconfig/network-scripts/ifcfg-ens32 /etc/sysconfig/network-scripts/ifcfg-eth0
mv /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-eth1

-control1
echo -e "TYPE=Ethernet\nBOOTPROTO=none\nNAME=eth0\nDEVICE=eth0\nONBOOT=yes\nIPADDR=10.5.116.100\nPREFIX=8\nGATEWAY=10.0.0.1\nDNS1=8.8.8.8\nNM_CONTROLLED=no">/etc/sysconfig/network-scripts/ifcfg-eth0 
echo -e "TYPE=Ethernet\nBOOTPROTO=none\nNAME=eth1\nDEVICE=eth1\nONBOOT=yes\nIPADDR=192.168.1.100\nPREFIX=24\nNM_CONTROLLED=no">/etc/sysconfig/network-scripts/ifcfg-eth1

-compute1
echo -e "TYPE=Ethernet\nBOOTPROTO=none\nNAME=eth0\nDEVICE=eth0\nONBOOT=yes\nIPADDR=10.5.116.101\nPREFIX=8\nGATEWAY=10.0.0.1\nDNS1=8.8.8.8\nNM_CONTROLLED=no">/etc/sysconfig/network-scripts/ifcfg-eth0 
echo -e "TYPE=Ethernet\nBOOTPROTO=none\nNAME=eth1\nDEVICE=eth1\nONBOOT=yes\nIPADDR=192.168.1.101\nPREFIX=24\nNM_CONTROLLED=no">/etc/sysconfig/network-scripts/ifcfg-eth1

-compute2
echo -e "TYPE=Ethernet\nBOOTPROTO=none\nNAME=eth0\nDEVICE=eth0\nONBOOT=yes\nIPADDR=10.5.116.102\nPREFIX=8\nGATEWAY=10.0.0.1\nDNS1=8.8.8.8\nNM_CONTROLLED=no">/etc/sysconfig/network-scripts/ifcfg-eth0 
echo -e "TYPE=Ethernet\nBOOTPROTO=none\nNAME=eth1\nDEVICE=eth1\nONBOOT=yes\nIPADDR=192.168.1.102\nPREFIX=24\nNM_CONTROLLED=no">/etc/sysconfig/network-scripts/ifcfg-eth1


echo "NM_CONTROLLED=no" >>/etc/sysconfig/network-scripts/ifcfg-eth0 
echo "NM_CONTROLLED=no" >>/etc/sysconfig/network-scripts/ifcfg-eth1

grub2-mkconfig -o /boot/grub2/grub.cfg
reboot

yum -y install vim curl wget git net-tools
yum -u update

echo "alias vi='vim'">> /root/.bashrc
su

-------------------------------------------------------------------------------------
-control 노드 작업

1.오픈스택 버전 rocky를 설치하기위한 저장소목록 설치
yum -y install centos-release-openstack-rocky

yum -y update

2.스크립트 툴을 이용한 자동설치를 위해 packstack을 설치
yum -y install openstack-packstack

3.packstack을 이용하여 설치를 위한 스크립트를 작성

packstack --gen-answer-file=answer.txt

4. answer.txt 편집
-------------------------------------------------------
CONFIG_LBAAS_INSTALL=n > y
CONFIG_HEAT_INSTALL=n > y
CONFIG_MARIADB_PW=12e0da3c30f14814 > test123
CONFIG_KEYSTONE_ADMIN_PW=262d8283aec248ad >test123
CONFIG_KEYSTONE_DEMO_PW=bb708c077c304348 > demo

STORAGE SAHRA > 192.168.1.x로 바뀐것 확인

1193 번 , 1242 번라인 ip대역을 10.5.116.0/8 로 변경

 sed -i "s/10.5.116./192.168.1./g" answer.txt

CONFIG_COMPUTE_HOSTS=192.168.1.100 > 192.168.1.101,192.168.1.102
CONFIG_NETWORD_HOSTS=192.168.1.100
--------------------------------------------------
5.설치하기 

packstack --answer-file=answer.txt

설치완료 후 ls해보면 토큰 파일 두개가 보임

6. 작동확인하기 

source keystonerc_admin 혹은
source keystonerc_demo 를통해 오픈스택 접속

openstack network list
openstack router list
openstack image list
openstack server list

mysql -u root -ptest123을통해
db 구성 정보를 확인가능

7. 토큰 유효시간 편집

vi /etc/openstack-dashboard/local_settings
-----------------------------------------
SESSION_TIMEOUT = 14400
------------------------------------------


 vi /etc/keystone/keystone.conf
------------------------------------------
[TOKEN]
~~~
~~~
~~~
expiration=14400
----------------------------------------------

source keystonerc_admin
systemctl restart httpd 로 설정을 적용하기

웹브라우저에서 192.168.1.100(backend 주소) 로 접속하기

메인로고 : /usr/share/openstack-dashboard/static/dashboard/img/logo-splash.svg


네트워크를 지우려면 먼저 라우터를 지워야한다.

생성시에는 먼저 네트워크를 생성한뒤
게이트웨이ip를 라우터의 ip로 지정한다

외부네트워크의 생성은 Admin만 가능하다


네트워크 생성시  게이트웨이 IP지정하지않고 넘어가면
자동으로 대역의 1번 ip가 지정

=keystone 

사용자 > 로그인> project (tenent :클라우드 환경) > 멀티테넌시 (한공간에 여러테넌트가 서로에게 영향을 주지않고 공존하는 공간)
> keystone 이 인증성공! > 사용자는 클라우드 환경을 이용할수 있는 TOKEN을 발급받는다
---end point (서비스와 사용자를 연결하는 주소) < ==== > 서비스 


역할 : 로그인에 성공한 사용자는 클라우드 환경내에서 이용 또는 제어가능 한 서비스의 범위를 부여받는다.
role(역할) > RBAC(Role Based Access Control )

도메인 : 사용자 , 그룹 + 프로젝트 

Key stone 서비스 > 사용자 , 그룹 프로젝트 등을 모두 MariaDB에 저장하고 인증수행은 LDAP를이용함


-glance에 이미지 등록
https://docs.openstack.org/image-guide/obtain-images.html에서
원하는 이미지를 찾아서 다운로드하여 GUI메뉴에서 업로드
혹은
CLI에서 wget을 통해 받아와서 openstack image create로 추가

ex) 
openstack image create "UBUNTU1804" --file bionic-server-cloudimg-amd64.img --disck-format qcow2 --container-format bare --public


cirros이미지를 사용하여 인스턴스를 두개생성후
서로 ping이 가는지 확인하고 유지 시켜두기

cli 로 돌아와서 openstack server list로 ID확인
openstack server show ID 로 현재 어느 노드에 배치되어있는지 확인
> 각 한개씩 compute1,2에 배치

live migration실행
openstack server migrate 4eb892b7-481e-4bf5-91ba-507300e061a9 --live compute2 --block-migration
공유 스토리지로 연결되어있는 경우에는 라이브마이그레이션이 정상적으로 동작하나
공유스토리지가 없는 경우 디스크와 xml파일을 전체 다 옮겨야한다
(기본 migration은 xml만 이동 --block-migration은 disk를 옮김)

==사용자추가
그룹(dev) > 사용자(devuser1, devuser2) using cli

openstack gui페이지에서 사용자생성 > openstack cli사용불가
cli에서 keystone을 복사해 수정하면 사용가능

-새로운 프로젝트 생성
openstack project create --domain default --description "신한은행 SOL 프로젝트" sol

-새로운 사용자생성, 프롬프트를 열어서 사용자의 암호를 설정
openstack user create --domain default --password-prompt dev1

 -사용자를 sol이라는 프로젝트에 _member_롤로 추가
 openstack role add --project sol --user dev1 _member_

 ==Neutron(네트워크)

 Neutron에서 제공하는 대표적인 서비스들

 1.VPN : 재택근무자가 외부에서 회사내부의 컨트롤러로 접속하기 위한 용도
(SSLVPN, 사용자가 VPN GATEWAY에 접속)

본사 지사간 연결 - IPSec VPN [site-to site VPN])


 2.firwall
 3.DHCP 
 4.LB 
 5.Switch(linux bridge , OPenVswitch), Router(Routing)
- overlay 네트워크

      linux bridge   vs openvswitch
       L2 only                   L4
                                        Routing , QoS, ACL
                                        터널링(gre,vxlan)-> overlay 네트워크 만들 수 있다.
                                        SDN 을 지원한다.


  SDN은 스위치 내부에 있는 data plane/control plane 을 분리하여 운영할 수 있다.
 data plane : MAC 테이블 처리 영역
 control plane : 라우팅, QoS, ACL 등의 처리 가능 

openstack 은 기본적으로 openvswitch 의 VxLAN 기술을 사용하여 터널링을 제공한다.
여러 compute 노드는 VxLAN 을 이용한 터널링이 지원되므로 인스턴스의 이동(마이그레이션)이 자유롭다





외부 네트워크 구성을 위한 설정과 연결
1. openvswitch 기반의 브릿지 : br-ex
eth0 이 br-ex 의 하나의 포트로 동작하도록 설정해 주어야 한다.
eth0 이 현재는 L3 -> L2로 전환해야 한다

해야할 일
br-ex   생성(openvswitch기반)
기존 eth0 은 br-ex 의 하나의 L2포트로 동작해야 한다.

 [root@control network-scripts(keystone_admin)]# cat ifcfg-br-ex
TYPE=OVSBridge
BOOTPROTO=none
NAME=br-ex
DEVICE=br-ex
DEVICETYPE=ovs
ONBOOT=yes
IPADDR=10.5.116.100
PREFIX=8
GATEWAY=10.0.0.1
DNS1=8.8.8.8
NM_CONTROLLED=no

[root@control network-scripts(keystone_admin)]# cat ifcfg-eth0
TYPE=OVSPort
BOOTPROTO=none
NAME=eth0
DEVICE=eth0
DEVICETYPE=ovs
OVS_BRIDGE=br-ex
ONBOOT=yes
NM_CONTROLLED=no
[root@control network-scripts(keystone_admin)]# systemctl restart network

[확인]
# ovs-vsctl show
   [생략]
    Bridge br-ex
        Controller "tcp:127.0.0.1:6633"
            is_connected: true
        fail_mode: secure
        Port phy-br-ex
            Interface phy-br-ex
                type: patch
                options: {peer=int-br-ex}
        Port br-ex
            Interface br-ex
                type: internal
        Port "eth0"
            Interface "eth0"


    [생략]

2. "1" 에서 생성된 br-ex 를 외부 연결용 브릿지로 지정하고 기 생성해 두었던 라우터와 연결하여

  인터넷(10.0.0.1)--eth0(br-ex 브릿지)가상포트---ADMIN_R1---| 172.16.1.0/24
 
외부네트워크 연결을 위해서는 eth0 가 extnet 이라는 physical network 이고 이 extnet 을 extnet:br-ex 형태로 매핑해야 한다. 

-플레이버 생성
openstack flavor create --id 6 --vcpus 1 --ram 1024 --disk 10 m1.xsmall
openstack flavor create --id 7 --vcpus 2 --ram 1024 --disk 10 m1.xsmall2
-외부네트워크 생성 및 서브넷 생성

openstack network create --provider-physical-network extnet --provider-network-type flat --external ext_net

openstack subnet create ext_net_subnet --network ext_net --subnet-range 10.5.116.0/8 --allocation-pool start=10.5.116.101,end=10.5.116.199 --gateway 10.0.0.1 --dns-nameserver 8.8.8.8 --no-dhcp

openstack network set --external ext_net

*overlay로 구성되었다 = vxlan을 통해 통신한다

*오버레이 네트워크, 클라우드 환경에서 vlan을 사용할수없는이유

1.vlan은 L3구간을 넘어갈수없으므로 두개이상의 인스턴스가 서로다른 지역의
서버에 배치되어있다면 이를 하나의 네트워크로 묶을 수없다.

2.vlan은 0~4095까지 4096개만 사용이가능 만약 이를 이용하여 오버레이 구성시
4096명의 고객만을 유치할수있음

3.VXLAN은 오버레이를 위한 터널링이 지원, 두개이상의 인스턴스가 서로 다른 지역의
물리서버 ( compute node) 에 배치되어있다고 하더라도 하나의 네트워크로 묶을 수있다.
VxLAN은 1600만개를 사용할수있으므로 다수의 고객을 유치할수있음


*ML2플러그인
오픈스택 내부에 flat,vxlan,vlan,gre등과 같은 다양한 네트워크 환경구성요소를
서로간의 통신이 가능하도록 지원해주는 플러그인

이를 통해 서로 다른 네트워크타입에 속해있는 인스턴스간 통신이 가능해진다.

iscsi : ip를 이용한 원격방식의 scsi


flavor 지정하기

[root@control ~(keystone_admin)]# openstack flavor create --id 6 --vcpus 1 --ram 1024 --disk 10 m1.xsmall
[root@control ~(keystone_admin)]# openstack flavor list
+----+-----------+-------+------+-----------+-------+-----------+
| ID | Name      |   RAM | Disk | Ephemeral | VCPUs | Is Public |
+----+-----------+-------+------+-----------+-------+-----------+
| 1  | m1.tiny   |   512 |    1 |         0 |     1 | True      |
| 2  | m1.small  |  2048 |   20 |         0 |     1 | True      |
| 3  | m1.medium |  4096 |   40 |         0 |     2 | True      |
| 4  | m1.large  |  8192 |   80 |         0 |     4 | True      |
| 5  | m1.xlarge | 16384 |  160 |         0 |     8 | True      |
| 6  | m1.xsmall |  1024 |   10 |         0 |     1 | True      |
+----+-----------+-------+------+-----------+-------+-----------+
[root@control ~(keystone_admin)]#


VNC설정

   공인IP:59XX -----------> 10.5.1.X:5900

    국정완 : 5901
        세흥 :5902
        준혁 :

xsmall 로 인스턴스 만든 뒤, ssh 인증할 때 계정은 
ubuntu 를 입력하면 사용자의 private 키를 서버의 public 키와 매칭시킨 뒤 정상적인 키라면 연결에 성공합니다.

설정에 문제가 없는데 연결이 잘 안되는 분들은 xsmall 로 만든 인스턴스를 삭제하고
small 로 해서 다시 만들어 보세요~ 잘 되네요

인증에 성공하면 
sudo passwd root
test123
test123
su root
apt-get install -y apache2
해서 웹 서버 설치하고 외부에서 floating ip 로 웹 접속 해 보세요!


CLI 로 인스턴스 생성하기
[root@control ~(keystone_admin)]# openstack server create --flavor m1.small --image UBUNTU1804 --security-group WEBSRV_SG --key-name ubuntu1804 --nic net-id=ca961c3b-968e-476e-8d55-5b6c38bc585a WEBSRV2

-임시로 floating ip 를 2번째 인스턴스에 할당
-외부에서 ssh 로 접속하여 apache2 설치한 뒤, 간단히 index.html 파일을 수정
-두 우분투 서버는 sudo passwd root 로 루트 패스워드 미리 지정해 두고
- 웹서버 설치가 끝나면 둘 다 floating ip 는 해제 한뒤, 서버끼리 서로 
   curl 을 이용하여 상대의 웹서버 동작 상태를 확인한다.

인스턴스 (qcow2 -> centos7)
- virt-customize 를 이용하여 실행시 자동으로 docker 가 설치되어 있어야 한다.
- 도커 버전은 docker-ce  최신 버전
- httpd 가 설치되어 있어야 한다.  
- size 는 지정하지 말것!!!


wget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-2003.qcow2.xz
xz -d CentOS-7-x86_64-GenericCloud-2003.qcow2.xz


이미지 생성 -> ftp 에 업로드 -> 스냅샷(오픈스택 설치완료:eth0,eth1) 
 -> answer.txt 파일 이용하여 heat 설치
 -> 외부 네트워크 설정 (br-ex)
 -> heat 의 yml 파일 작성
 -> 만들어둔 이미지 테스트 하기
 -> heat 파일 이용하여 배포하기
-------------------------- 
오후 : GCP 설정 
구글 클라우드에 도커 클러스터 환경 구현(kubernetes 이용)
1. gcp
2. 로컬에 설치(manager,worker)

 k8s 는 외부 노출이 불가능하다. 노출하려면 service(네트워크 서비스) 를 이용해야 한다.
- cluster ip
- nodeport
- LB


ansible + openstack
ansible + docker
ansible + aws | gcp 

"오케스트레이션 tool" -> 한번에 인프라와 인스턴스 | container를 생성 및 배포 하고 이를 연결하여  즉시 서비스가 가능한 상태를 만들 수 있는 것. 
생성된 후에는 인스턴스의 배포나 인프라의 변경이 수월하다. 

-------------------------------------------------------------
개인프로젝트 : 

controller : kubernetes - master node (TOKEN 발행)
                                                   worker node (토큰 삽입)


설치가 마무리되면 외부 연결을 위한 br-ex 설정을 진행해야 한다.
1. 미리 설치되어 있는 라우터와 두개의 네트워크 삭제(라우터를 먼저 삭제해야 함) - 관리에서 
2. 외부 네트워크(br-ex) 설정
3. 내부 네트워크 설정(private01 : 172.16.1.0/24)
4. 라우터 생성 하면서 외부네트워크(게이트웨이 설정)와 연결하고 인터페이스에서 내부 네트워크 역시 추가해 주어야 한다.

              br-ex |----Router-----| private01

5. 인우씨 서버에서 WINscp, powershell  이용하여 이미지 다운로드
6. 해당 이미지로 배포테스트(인스턴스 설치 된 뒤에 웹서버 동작상태, 도커 실행 상태 확인)

------------ heat_test.txt 파일---------------------
heat_template_version: 2015-04-30       
                                          
resources:                              
  instance:                             
    type: OS::Nova::Server              
    properties:                         
      flavor: m1.small                
      image: ubuntu1804
      networks:                         
        - network: private01              
      key_name: test
      security_groups:
        - default
---------복잡한 환경 구성의 예----------------------------------
heat_template_version: 2015-04-30

parameters:
    key_name:
        type: string
        label: Key Name
        description: SSH key to be used for all instances
        default: your_key
    image_id:
        type: string
        label: Image ID
        description: Image to be used. Check all available options in Horizon dashboard or by using openstack image list command.
        default: Ubuntu 18.04 LTS (19.18)
    private_net_id:
        type: string
        description: ID/Name of private network
        default: private_network_0XXXX
    eodata_net_id:
        type: string
        description: ID/Name of eodata network
        default: eodata



resources:
        Group_of_VMs:
                type: OS::Heat::ResourceGroup
                properties:
                        count: 2
                        resource_def:
                                type: OS::Nova::Server
                                properties:
                                        name: my_vm%index%
                                        flavor: eo1.xsmall
                                        image: { get_param: image_id }
                                        networks:
                                                - network: { get_param: private_net_id }
                                                - network: { get_param: eodata_net_id }
                                        key_name: { get_param: key_name }
                                        security_groups:
                                                - allow_ping_ssh_rdp
                                                - default

        VOL_FAQ:
                type: OS::Cinder::Volume
                properties:
                        name: vol
                        size: 20
                        image : { get_param: image_id }


        With_volume:
                type: OS::Nova::Server
                properties:
                        flavor: eo1.xsmall
                        block_device_mapping: [{"volume_size": 20, "volume_id": { get_resource: VOL_FAQ }, "delete_on_termination": False, "device_name": "/dev/vda" }]
                        networks:
                                 - network: { get_param: private_net_id }
                                 - network: { get_param: eodata_net_id }
                        key_name: { get_param: key_name }
                        security_groups:
                                 - allow_ping_ssh_rdp
                                 - default
                        image : { get_param: image_id }

outputs:
        SERVER_DETAILS:
                description: Shows details of all virtual servers.
                value: { get_attr: [ Group_of_VMs, show ] }     
 



------------GCP
새프로젝트를 만들고 프로젝트 id복사후 쉘을 열어 

gcloud config set project wocheon-keduit

-구글 GCP SDK
내가가진 리눅스와 GCP의 콘솔을 연결가능 

프로젝트 생성후 활성화 시켜야할 API목록

GCP SDK 활성화 > api에서 
Cloud Source Repositories API,
Kubernetes Engine API, 
Google Container Registry API
Cloud Build API
를 활성화하기


- Cloud Build API : 기존의 도커의 경우 이미지 생성 -> 업로드의 작업이 분리되어 있으나
Cloud Build 를 사용하면 이미지 생성 부터
GCP 내의 사설 저장소에 이미지를 업로드 하는 것 까지 한번에 진행할 수 있다.

-Cloud Source Repositories API : github와 같이 소스코드를 저장, 관리할수있는 GCP내의 코드저장소

-Kubernetes Engine API : 쿠버네티스 클러스터 연결을 위한 API

-Container Registry API : GCP내에 이미지를 저장 관리할수 있는 사설저장소 


쿠버네티스의 기본 container 관리도구 : docker 
gcloud config set project ciw0707-0517

daskete07@cloudshell:~ (graphite-byte-313706)$ gcloud config list project
[core]
project = graphite-byte-313706

daskete07@cloudshell:~ (graphite-byte-313706)$ gcloud config list project --format "value(core.project)"
graphite-byte-31370

>본인의 프로젝트 ID값을 뽑아내기

daskete07@cloudshell:~ (graphite-byte-313706)$ export PROJECT_ID=$(gcloud config list project --format "value(core.project)")
daskete07@cloudshell:~ (graphite-byte-313706)$ echo $PROJECT_ID
graphite-byte-313706

> 변수에 할당시키기

git clone https://github.com/beomtaek78/btstore
cd btstore/kube/

zone (도시): 실제 물리적인 DC의 위치를 의미 
region (국가): DC를 연결하여 이중화된 클러스터 환경을 구축하는 논리적 그룹

   36  gcloud container clusters get-credentials cluster-3 --zone asia-northeast3-b --project wocheon-keduit
   37  kubectl get node
   38  kubectl get pods
   39  kubectl get svc
   40  kubectl get rs
   41  kubectl get deploy
   42  kubectl get namespace


===쿠버네티스(k8s)용어정리 

pod : container 1개이상을 묶어 서비스를 제공(상위개념 : 레플리카셋)
반드시 동일한 노드상에 동시에 전개된다는 특징을 가짐


레플리카셋 replicaset 5 5-1 +1  고정된 pod의 개수를 지정한다 (상위개념 : deployment)

deployment : rs,pod의 최상위개념, 롤링업데이트(사용중에 버전 업데이트 가능)을 지원

서비스 - 클러스터 내부의 pod간 통신과 pod의 외부로의 노출을 위한 설정을 제공

namespace -작업공간의 분리를 담당

etcd : 클러스터 전체 구성도를 관리 > 일반적으로 별도 구축없이 master내에 구축

master가 한대인경우 master가 다운되면 클러스터 전체구성의 확인 및 명령을 내릴수없으므로
클러스터 동작이 불가능 , 이로 인해 master는 2대이상을 구성하는 것이 일반적
또한 etcd도 전체 클러스터 정보가 포함되어있으므로 2대 이상을 구성해주는 것이 좋다


replicaset으로 auto-scale 구현가능 > 
의미하는것은 노드의 cpu ram등을 확인하여 자동으로 확장 수축하는것이 아닌replicaset 개수를 3에서 5로 지정하면
자동으로 pod가 5로 늘어나는 것을 의미한다, "scaling구현 가능하다" 생각 하는 것이 더 적절함


서비스 : pod로의 접속혹은 외부에서의 접근을 위한 서비스
- cluster-ip : 외부로 노출되지 않는다. 클러스터 내부에서만 통신할 때 사용하는 IP
 
- node-port : 외부 노출 가능. 트래픽 분산은 되지 않는다. 
- LB : 일반적으로 기업에서 가장 많이 사용하는 서비스이며 트래픽 분산이 가능하다
             on-premise 에서는 별도의 애플리케이션을 사용하여 기능을 구현하며, 퍼블릭 클라우드에서는 GCP,AWS  의 LB 를 바로 연결하여 사용할 수 있다. 

Label
pod 단위로 label 을 붙여 트래픽 분산 또는 지정된 pod로의 접속을 유도할 수 있다.

kubectl get svc
         cluster-ip

-----------GCP실습

-클러스터만들기

리전 : northeast3중 하나 
출시버전 : 기본
노드 이미지 : docker가 설치된 ubuntu
E2-medium
부팅디스크 크기 : 100G
노드수 3 개


-ubuntu와 GCP연동하기
ubuntu를 인터넷 연결 가능하도록 설정후
apt-update 
snap install google-cloud-sdk --classic
gcloud init 

*명령어가 없는 경우 
PATH=$PATH:/snap/bin

절차대로 진행후 링크가 뜨면 접속해서 로그인하기
완료후 에는 

snap install kubectl --classic

gcloud container clusters get-credentials cluster-1 --zone asia-northeast3-a --project ciw0707-0517

gcloud container clusters get-credentials cluster-1 --zone asia-northeast3-a --project gcp-in-ca

kubectl get nodes
로 노드 목록 확인해보기 

kubectl get pod -n kube-system

cd 
mkdir k8s ; cd k8s

git clone https://github.com/beomtaek78/btstore
cd btstore/kube/


sudo apt install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
sudo apt update
apt-cache policy docker-ce

gcloud build submit --config config/cloudbuild.yaml 

kubectl api-resources
kubectl get deploy

configmap > etcd에 담아둔다 이는 master,manager가 etcd와 직접 통신ㅇ하므로 
master는 각 pod들의 변수 값을 한번에 확인할수있다. configmap의 값들은 모두 평문으로 저장
pod들이 공통적으로 사용하는 변수등을 지정할때 사용한다

secret 
각 pod에서 사용하는 패스워드 api연결을 위한 키값등을 배포할때 사용하며
암호화되어저장되어 볼수없다


kubectl apply -f config/configmap.yaml
kubectl apply -f config/secrets.yaml


pod, 레플리카 셋, 디플로이 먼트 등을 배포...
1. yml 파일 만들기
2. kubectl apply(또는 create) -f test.yml 
3.(삭제) kubectl delete -f test.yml 


yaml파일 만들기위해서는
kubectl apply -f xx.yaml 


vi config/deployment-blue, green 파일 수정 후 배포
---------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      color: blue (green은 그린으로)
    spec:
      containers:
      - image: gcr.io/ciw0707-0517/imageview:blue
                        [PROJECT_ID]
------------------------------------------------------

kubectl apply -f config/deployment-green.yaml --validate=false
로 배포하기 

kubectl get pod로 확인가능

kubectl describe pod webserver-blue-88df46c95-2hhdm |grep IP
로 pod의 IP확인하기

kubectl run -it --image=centos:7 bash
로 다른 하나의 pod만들기

curl 10.32.1.4 아까 확인한 pod의 ip의 웹서버 동작 확인하기

>내부에서는 접근이 가능하지만 외부에서는 아직접근이 불가능함 

kubectl delete pod bash

yaml파일로 배포한 pod를 삭제시 
> kubectl delete -f config/deployment-blue.yaml

kubectl apply -f config/service.yaml로 로드밸런서 배포

GCP페이지에서 클라우드 엔진 > 서비스 및 수신 목록에
생성이 되었는지 확인하고 엔드포인트로 접속해보기

yaml메뉴를 눌러서 blue를 green으로 변경하면 변경사항적용



-QUIZ 

1.  nginx 이미지와 httpd(/var/www/html이 기본 디렉토리 아님!) 이미지를 본인의 사설 저장소에 업로드 하되, 아래의 내용을 포함해야 한다.(build 를 이용할 것)

      nginx 의 기본 홈 디렉토리에 index.html ( HELLO NGINX)
      httpd 의 기본 홈 디렉토리에 index.html (HELLO HTTPD)

2. nginx 로 만든 deployment 는 label 을 web: nginx, httpd 로 만든 deployment 는 label 이 web: httpd 여야 한다. 둘다.. 공통 라벨로 system: server 가 붙는다

3. service /LB 이용하여 nginx 가 기본적으로 외부에서 접속 가능해야 한다.
     라벨 셀럭터를 web: httpd 로 하면 httpd 페이지가 열려야 한다.

>파일은 github확인

docker build -t gcr.io/ciw0707-0517/imageview:nginx ./nginx.d/
docker build -t gcr.io/ciw0707-0517/imageview:httpd ./httpd.d/

gcloud builds submit --config ./cloudbuild.yaml 

kubectl create -f deployment-httpd.yaml
kubectl create -f deployment-nginx.yaml
kubectl create -f service.yaml


--------로컬에서 GCP 이미지 저장소로 이미지 push하기

docker pull centos:7

docker tag centos:7 gcr.io/ciw0707-0517/mycentos:1.0

gcloud docker -- push gcr.io/ciw0707-0517/mycentos:1.0


kubectl get ns


-kubectl 자동완성 켜기 
echo "source <(kubectl completion bash)" >> ~/.bashrc
source ~/.bashrc 

kubectl api-resources > yaml파일작성시 참고

-----------pod의 레플리카 조정하기

1. replicaset이나 Deployment의 메니페스트 파일 내에서 
replicas 부분을 조정하고 해당파일을 다시 apply하기 
> 조정된 레플리카수가 적용됨 

2.kubectl  scale --replicas=5 -f test.yaml 



NoSQL : key:value 형태로 저장 




----------------k8s(kubernetes) 아키텍쳐-------------------------

master node 
-api-server : 컨트롤 플레인(k8s의 제어판)의 front-tend, 관리 개발자와 상호통신

-key-value store(etcd) : 클러스터 환경에 대한 구성정보를 담은 DB 
                                       마스터노드, pod, container들의 상태정보를 확인할 수 있다.

-controller : 클러스터의 실행, 하나의 컨트롤러는 스케줄러를 참조하여 정확한 수의 pod실행 
                    만약  pod에 문제 발생시 다른 컨트롤러가 이를 감지하고 대응함

-scheduler : 클러스터의 상태가 올바른가, 새로운 container 요청이들어오면 이를 어디에 배치할것인가? 등의 결정 담당

                                  
worker node
(master 로부터 업무를 전달받아 이를 처리하고 결과를 master에게 보고)

-networkproxy(kube-proxy) : 네트워크 통신담당 (다양한 모듈이있으므로 선택해서 설치해야함)

-kubelet : 컨트롤러에서 노드에 작업을 요청하면 kubelet이 이를 받아 처리

-runtime : kubelet 으로부터 작업을 받아 실제 container를 만드는 작업을 하는 도구 (docker)

추가요소 

-DNS : 각 pod에 대해 내부적으로 사용할수있는 도메인이름을 할당하고 IP와 매핑하여 처리

-Persistent Storage : 사용자가 기본스토리지 인프라에 관한 정보를 몰라도
                                  리소스 요청이 가능하다.
> 개발자는 서버에 필요한 추가디스크를 서버에 연결하기위해 nfs,iscsi,fc등의 기술을 몰라도
요청내용 중 간단한 내용만을 요청하면, 관리자가 미리 풀에 만들어둔 디스크와 매핑되어
자동으로 해당볼륨을 사용할수 있도록 해주는 기술


-ubuntu를 이용한 kubernetes cluster 구축

우분투 설치 완료후
- 모든 노드에서
apt-get install -y git curl wget vim

vim ~/.bashrc 
84번째 줄에 아래의 내용을 추가한다
alias vi='vim'
저장후 빠져 나와서 "su"

vi /etc/hosts 에 아래 내용을 추가

211.183.3.100  master
211.183.3.101  node1
211.183.3.102  node2
211.183.3.103  node3

- node1 에서만
systemctl set-default multi-user.target


- 모든노드에서
스왑메모리를 허용하지 않는다. 

swapoff -a

- master 에서만 vmware tool 설치
VM 메뉴에서 -> install vmware tools... 클리

cp /media/docker/VM[tab]/VM[tab] .
tar xfz VM[tab]
cd vmware-too[tab]
./vmware-install.pl

y <-- 처음만 y 나머지는 모두 그냥 엔터


이제 각 노드별(master, node1) 로 IP, GW, DNS 를 설정한다.

node1 -> systemctl set-default graphical.target

apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
apt-cache madison docker-ce 
apt-cache madison docker-ce-cli	
apt-get install docker-ce=5:18.09.9~3-0~ubuntu-bionic docker-ce-cli=5:18.09.9~3-0~ubuntu-bionic containerd.io
docker --version
apt-get update

여기까지 정상적으로 완료되었다면 이제 node1 을 종료하고

node1  을 복사해서 node2, node3 을 만든다.  : 복사할 때 "Create a full clone" 으로 복사하세요!!!!!!

단, node2, 3 을 실행하여 아래의 내용을 점검해야 한다.
1. hostname 변경하기
2. node2 -> 211.183.3.102
     node3 -> 211.183.3.103
3. /etc/hosts 에 들어가서 node1 이라고 되어 있는 부분
127.0.1.1       node1
위의 부분을 각각 node1, node2 로 바꾼다

4. node1 ~ 3 까지는 모두 "systemctl set-default multi-user.target" 
5. 재부팅!!!
 
위의 설정이 완료되었다면 모든 노드를 ssh  로 연결해 두세요!!!
-------------- 점검사항------------------------
각 노드에서 서로간에 ping 이되는지 여부 확인
특히 각 노드에서 master 로는 반드시 ping 이 가능해야 한다.

--------------------------------------------
쿠버네티스 초기화

모든 노드에서 swapoff -a 

master 에서 아래의 명령을 실행하면 토큰이 발행된다.
kubeadm init --apiserver-advertise-address 211.183.3.100 --pod-network-cidr=192.168.0.0/16

kubeadm init --apiserver-advertise-address 10.178.0.9 --pod-network-cidr=10.178.0.0/32

예)
kubeadm join 211.183.3.100:6443 --token 1by71b.fi41vzfwfhej3du6 \
        --discovery-token-ca-cert-hash sha256:2beab8e2f7ebaa5364a22b8bc78d2f6ab5aecb358eef641076b9eeff618fdb3c

위와 같은 토큰이 발행되고 이 토큰을 이용하여 master 에 join 하게 된다. 방화벽에서 해당 포트를 열어주어야 한다. 단 향후 서비스의 편의를 위해 모든 노드에서 ufw disable 을 한 뒤에 위의 토큰을 각 노드에 붙여넣기 한다.

이후 역시 master 노드에서 아래의 내용을 실행하여 초기화를 마무리 한다.
root@master:~# mkdir -p $HOME/.kube
root@master:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
root@master:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config
root@master:~# export KUBECONFIG=/etc/kubernetes/admin.conf
root@master:~#
root@master:~# kubectl get node
NAME     STATUS     ROLES                  AGE     VERSION
master   NotReady   control-plane,master   4m18s   v1.21.1
node1    NotReady   <none>                 73s     v1.21.1
node2    NotReady   <none>                 59s     v1.21.1
node3    NotReady   <none>                 57s     v1.21.1
root@master:~#


이제 오버레이 네트워크를 위한 매니페스트 파일을 master 에서 설치하면 끝!!!
kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml

 시간이 조금 지난 뒤에 master 에서 아래를 실행해 본다.

root@master:~# kubectl get node
NAME     STATUS   ROLES                  AGE     VERSION
master   Ready    control-plane,master   11m     v1.21.1
node1    Ready    <none>                 8m22s   v1.21.1
node2    Ready    <none>                 8m8s    v1.21.1
node3    Ready    <none>                 8m6s    v1.21.1


-------------------------------------------------------------------------------------

==쿠버네티스의 사용

-퍼블릭클라우드 환경에서의 구성 : GKE, EKS
-온프레미스에서의 구성 : Kubeadm 

-master : 전체 클러스터 관리 및 배포
-nodes (worker) : master로부터 작업을 지시받고 해당업무를 수행하여 container를 배포나

쿠버네티스 토큰은 만료기간이있음 : 일정시간동안 join하지않으면
또 다른 토큰을 발행해주어야한다.

kubeadm token list

token 뒤에는 ca-sha256으로 publickey를 hashing하여 붙여넣음
> node에서 master로 접속하려면 token 과 public키 두개가 필요하다

/etc/kubernetes/pki/ca.crt 파일을 아래의 shell 을 이용하여 해시할 수 있다.
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'

kubeadm token create --print-join-command

kubectl get svc
> external-ip : <none> : 외부에서는 접근불가능함

kubectl get pod
kubectl get deploy
kubectl get rs

cluster ip : POD간의  연결을 위한 ip, kubernetes내부에서만  pod들에 접근가능
  외부로의 연결은 불가능 

각 pod는 하나의 IP를 가지며 pod내의 container들은 하나의 IP를 공유하여 사용한다.

pod(pod) : 서비스제공의 최소단위, 1개이상의 container로 구성
일반적으로 1개의 container로 구성하고 log,모니터링, 백업 등을 위해
side-carcontainer를 사용할수있음 

일반도커 container와 달리 발행 즉시 외부로의 연결이 되는 것이 아니다.

*side-car : 서비스를 제공하는데 영향을 주지않고, 없어도 상관은없으나
서비스에 도움을 줄수있는 container를 말함, 일반적으로 실제 제공하는 서비스와
하나의 pod로 구성되어 배포됨 

일반적으로 application의 배포는 yaml(or yml)파일을 구성하여 배포 

==yaml파일을 통한 pod 배포하기 

vi nginxpod.yaml 
------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: my-first-nginx
spec:
  containers:
  - name: my-first-nginx-ctn
    image: nginx:1.10
    ports:
    - containerPort: 80
      protocol: TCP
---------------------------------------


kubectl apply -f  nginxpod.yaml

kubectl get pod

kubectl get pod -o wide
>pod에 대한 정보 확인 ( status,ip, 배포된 node위치)

kubectl describe pod my-first-nginx
>pod에 대한 더 자세한 정보보기 ( log, volume, ip, port 등등)

pod 삭제 방법
1. kubectl delete pod podname
2. kubectl delete -f name.yaml


==sidecar 를 포함한 pod만들기 

vi nginxpod.yaml 
------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: my-first-nginx
spec:
  containers:
  - name: my-first-nginx-ctn
    image: nginx:1.10
    ports:
    - containerPort: 80
      protocol: TCP

  - name: test-ctn
    image: centos:7
    command: ["tail"]  > 명령실행(ENTRYPOINT)
    args: ["-f", "/dev/null"] > 명령실행(CMD)

--------------------------------------------------------------------

****************************************************************************
만약 ENTRYPOINT 를 사용하여 container 수행 명령을 정의한 경우,
해당 container가 수행될 때 반드시 ENTRYPOINT 에서 지정한 명령을 수행되도록 지정 된다.

 하지만, CMD를 사용하여 수행 명령을 경우에는,
container를 실행할때 인자값을 주게 되면 Dockerfile 에 지정된 CMD 값을 대신 하여 지정한 인자값으로 변경하여 실행되게 된다.

container가 수행될 때 변경되지 않을 실행 명령은 CMD 보다는 ENTRYPOINT 로 정의하는게 좋다.
메인 명령어가 실행시 default option 인자 값은 CMD로 정의해 주는게 좋다.
ENTRYPOINT 와 CMD는 리스트 포맷 ( ["args1", "args2",...] )으로 정의해 주는게 좋다.
********************************************************************************

kubectl get pod -o wide

해당 노드에서 
docker contanier ls 로 확인해보기 

kubectl exec -it  my-first-nginx -c test-ctn bash
로 centos7로 접속 

pod는 도커의 container/stack과 달리 pod내부에있는 container들이
pod의 자원을 공유하여 사용한다.
> test-ctn 이 curl localhost를 통해 웹서버로 접속하면
80번포트에서 서비스하고있는 my-first-nginx-ctn으로 웹접속이 됨
결국 한 pod에 속한 두 container는 80번포트를 각각 사용할수없음


=replicaset
고정된수의 pod를 지속적으로 동작시키는 것
pod의 상위개념이면 pod의 설정값에 replica 부분이 추가되는것 

cp nginxpod.yaml nginxrs.yaml

vi nginxrs.yaml 
--------------------------------------------------------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-first-nginx-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      color: black
  template:
    metadata:
      name: my-first-nginx
      labels:
        color: black
    spec:
      containers:
      - name: my-first-nginx-ctn
        image: nginx
        ports:
        - containerPort: 80
          protocol: TCP

      - name: test-ctn
        image: centos:7
        command: ["tail"]
        args: ["-f", "/dev/null"]
----------------------------------------------------------------------------
kubectl get pod,rs

kubectl delete pod 로 pod를 하나 지우고 확인해보기
> 지워도 다시 새로운 pod가 생성된다

 kubectl get pod,rs

replicas 를 4로변경해서 다시 배포해보기 

kubectl apply -f nginxrs.yaml
kubectl get pod,rs

=Deployment
-pod > replicaset > Deployment
replicaset의 기능 + Rolling update
서비스의 중단없이 application을 업데이트할수있음
특정 시점을 스냅샷처럼 저장하여  롤백이 가능함

결국  서비스의 배포는 pod, replicaset의 구성은 필요x 
(Deployment만 구성해서 배포하면됨)

cp nginxrs.yaml nginxdeploy.yaml

vi nginxdeploy.yaml
----------------------------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-first-nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      color: black
  template:
    metadata:
      name: my-first-nginx
      labels:
        color: black
    spec:
      containers:
      - name: my-first-nginx-ctn
        image: nginx:1.10
        ports:
        - containerPort: 80
          protocol: TCP

      - name: test-ctn
        image: centos:7
        command: ["tail"]
        args: ["-f", "/dev/null"]

----------------------------------------------------------------------------------
> ReplicaSet과 차이점이 거의 없음 

kubectl apply -f nginxdeploy.yaml

kubectl get pod,rs,deploy

kubectl apply -f nginxdeploy.yaml --record
> 배포시점을 기록하기 

kubectl rollout history deployment
> 기록을 확인하기 

== (rolling update)application업데이트 방법
1. yaml파일을 수정하여 업데이트 하는 방법 
2. kubectl set image deployment 를 이용하여 업데이트 

kubectl set image deploy test-deploy test-ctn=nginx:1.12 --record

kubectl set image deployment/my-first-nginx-deploy my-first-nginx-ctn=nginx:1.16.1 --record

kubectl describe pod my-first-nginx-deploy-6d7f54dc65-2gw5m | grep nginx:1.

kubectl rollout undo deployment my-first-nginx-deploy --to-revision=2


-------------롤링업데이트 실습 --------------------
자신만의 이미지 만들기 준비


curl www.centos.org>index.html
vi 편집기로 The Centos Project 부분뒤에 version1붙여두기

Dockerfile 작성
-----------------------------------------------------------
FROM nginx:1.10
EXPOSE 80
ADD index.html /usr/share/nginx/html/index.html
CMD nginx -g 'daemon off;'
-----------------------------------------------------------

docker build -t testnginx:1.0 .
> 이미지 생성

docker container run -d  --name test -p 8001:80  testnginx:1.0
>이미지 테스트용으로 run으로 배포해보기 

docker tag testnginx:1.0 ciw0707/testnginx:1.0
> docker hub에 업로드를 위해 tag변경 (사전에 로그인필요)

docker push ciw0707/testnginx:1.0
>docker hub에 이미지 업로드

index파일의 version을 2.0으로 변경하여 testnginx:2.0 이미지를 만들기
같은 과정을통해 docker hub에 업로드

cp nginxdeploy.yaml mydeploy.yaml
>이전 실습에서 사용한 yaml파일을 참조하여 새로운 yaml파일 생성

vi mydeploy.yaml
---------------------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-test-nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      color: black
  template:
    metadata:
      name: my-test-nginx
      labels:
        color: black
    spec:
      containers:
      - name: my-test-nginx-ctn
        image: ciw0707/testnginx:1.0
        ports:
        - containerPort: 80
          protocol: TCP
----------------------------------------------------------------------------
 
 kubectl apply -f mydeploy.yaml
 >작성된 yaml파일을통해 Deployment 배포


서비스의 배포/접근을 위해서는 service를 사용해야한다.
서비스는 네트워크를 담당한다

-cluster-ip : 클러스터간 통신주소, -p옵션이 없는 docker run과 동일( 외부와 통신불가)

-node-port : 포드에 접근할수있는 포트를 클러스터의 모든노드에 동일하게 개방

-load balance(LB) : 일반적으로 사전에 LB가 준비되어있는 환경에서
연결할수있는 방법이며  yaml을통해 연결이 가능해야한다.
보통은 AWS,GCP같은 퍼블릭환경에서 사용하기 용이함
일반적인 환경이라면 node-port를 통해 충분히 서비스가가능함


각각의 기능을 object 라하고 그 오브젝트내에서 service를사용해야만 
외부노출,외부로부터의 접근이 가능해진다

vi servicecip.yaml
------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-svc
spec:
  ports:
    - name: webport
      port: 8080
      targetPort: 80
  selector:
    color: black
  type: ClusterIP
  -------------------------------------

  
vi nodeport.yaml
------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-svc
spec:
  ports:
    - name: webport
      port: 8080
      targetPort: 80
  selector:
    color: black
  type: NodePort
  -------------------------------------

kubectl apply -f nodeport.yaml 

kubectl get srv,deploy로 확인 

service/my-svc       NodePort    10.109.76.218   <none>        8080:30394/TCP   28s
> 각 노드의 ip:30394와 cluster-ip:8080가 연결됨(node port 는 랜덤으로 할당됨)
*master(211.183.3.100)으로 접속하여도 웹서버로 접속가능

-연결상태
nodeip :211.183.3.101 : 30394
                |
                |
Cluster IP : 10.109.76.218:8080
                |
                |
pod(s) IP : 192.168.x.x : 80 


-고정된 노드포트를 사용하기 

vi nodeport.yaml
------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-svc
spec:
  ports:
    - name: webport
      port: 8080
      nodePort: 30001
      targetPort: 80
  selector:
    color: black
  type: NodePort
------------------------------------

kubectl get srv,deploy로 확인 

kubectl describe service my-svc
> Endpoint를 통해 현재 연결되어있는 pod들을 확인 가능


HAProxy 서버를 만들어서 외부에서 접속할수있도록 연결해보기 

 실습 :
HAProxy 를 구성한다. 단, HAProxy 는 아래의 조건을 갖추어야 한다.

- OS : CentOS 7.0 (minimal install)
- cpu 2, RAM : 2GB , HDD : 20GB
- NIC : 
          ens32 : bridge                   -> 10.5.101/102/103/104. ____
          ens33 : VMnet8(NAT)   -> 211.183.3.99
 
웹 접속이 가능한 것을 확인 했다면 "롤업"을 하여 CentOS(Version 2) 가 보이도록 하세요

"External Traffic Policy: Cluster" 
: 노드에 생성된 포드가 존재하지 않더라도 클러스터 네트워크를 통해 타 노드로 연결을 유지하여 서비스를 제공해 주는 방식으로 
현재 상태에서는 211.183.3.100 에 웹서비스를 위한 포드가 존재하지 않아도 해당 주소로 웹접속시 페이지가 보이게 된다. 
Cluster 의 경우 이러한 편리함을 제공하지만 타 노드를 경유하여 서비스가 제공되므로 트래픽 부하가 내부적으로 증가할 수 있으며 
홉도 증가하게 되어 포드 입장에서는 접속자를 외부의 클라이언트가 아닌 타 노드의 IP로 오해할 수 있다.

만약 Cluster 가 아닌 "Local" 로 변경되면 "노드IP:노드Port"  로 접속했을 경우 
해당 노드에 생성된 포드로만 트래픽이 전달되고 타 노드로는 연결이 되지 않게 된다. 
결과적으로 211.183.3.100(master) 으로 웹 접속 시 실제로는 포드가 없으므로 웹 서비스를 제공받을 수 없게된다.

kubectl delete svc my-svc 
 > 기존 서비스 제거

vi serviceETP.yaml
----------------------------------

apiVersion: v1
kind: Service
metadata:
  name: my-svc-etp
spec:
  externalTrafficPolicy: Local       <--- 추가되었음
  ports:
    - name: webport
      port: 8080
      targetPort: 80
      nodePort: 30001
  selector:
    color: black
  type: NodePort
--------------------------------------------------------

externalTrafficPolicy
> 기본값은 Cluster( 클러스터로 연결되어있는상태에서 외부접근이 들어오는경우, 현재 pod가 없다면 다른 node의 pod로 넘겨주게된다 )
Local로 변경하면, 현재 해당노드에 pod가없다면 접속이 불가능, 다른 클러스터에 속한 node로 넘겨주지않는다


 kubectl scale --replicas=2 deploy my-test-nginx-deploy

 kubectl delete -f ./
 >현재 위치에 존재하는 yaml 파일을 참조하여 서비스,pod들을 삭제





 =두개이상의 서비스를 한번에 배포시키기 

 ---------------------------------------------------------------------------
 apiVersion: apps/v1
kind: Deployment
#replicaset
metadata:
  name: my-test-nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      color: black
#pod
  template:
    metadata:
      name: my-test-nginx
      labels:
        color: black
#container
    spec:
      containers:
      - name: my-test-nginx-ctn
        image: ciw0707/testnginx:2.0
        ports:
        - containerPort: 80
          protocol: TCP

---
apiVersion: v1
kind: Service
metadata:
  name: my-svc
spec:
  ports:
    - name: webport
      port: 8080
      targetPort: 80
      nodePort: 30001
  selector:
    color: black
  type: NodePort
---------------------------------------------------------------------------
 

=네임스페이스 ( name space ) 

-분리되어있는 별도의 작업공간, 포드, 레플리카셋, 디플로이먼트, 서비스 등과 쿠버네티스 리소스들이 묶여 있는 가상공간

-  --namespace 없을 경우 기본적으로 “default” 네임스페이스를 의미함

-kube-system 는 쿠버네티스 클러스터 구성에 필수적인 컴포넌트들과 설정값 등이 존재

-노드(nodes)는 쿠버네티스의 오브젝트 이지만, 네임스페이스에는 속하지 않는 오브젝트

-네임스페이스는 라벨보다 넓은 의미로 사용됨

-ResourceQuota 오브젝트 사용하여 특정 네임스페이스에서 생성된 포드의 자원 사용량 제한

-애드미션 컨트롤러라는 기능을 이용하여 특정 네임스페이스에서 생성되는 포드에는 항상
사이드카 컨테이너를 붙이도록 할 수 있다

-포드, 서비스 등의 리소스를 격리함으로써 편리하게 구분

-네임스페이스는 컨테이너의 격리된 공간을 생성하기 위해 리눅스 커널 자체를 사용

-일반적으로 네트워크, 마운트, 프로세스 네임스페이스등을 의미하며 리눅스 네임스페이스와는 별개


docker  : namespace- 작업공간 분리 , cgroup-리소스제한 


-namespace생성방법
1.yaml로 작성하기 

vi ns1.yaml 
-------------------------------
apiVersion: v1
kind: Namespace
metadata:
  name: develop
------------------------------

kubectl apply -f ns1.yaml 

kubectl get ns 

2.명령어로 생성하기 

 kubectl get ns
 kubectl get pod -n default

 kubectl create namespace testns
> metadata 쪽에 namespace로 지정하여 사용가능 


-특정 namespace에 리소스 배포하기 

vi mydeploy.yaml 
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-test-nginx-deploy
  namespace: develop
spec:
  replicas: 1
  selector:
    matchLabels:
      color: black
  template:
    metadata:
      name: my-test-nginx
      labels:
        color: black
    spec:
      containers:
      - name: my-test-nginx-ctn
        image: ciw0707/testnginx:2.0
        ports:
        - containerPort: 80
          protocol: TCP
----------------------------------------------

kubectl apply -f mydeploy.yaml 

kubectl get pod,deploy
> 기본적으로 default namespace에서 동작중인 리소스를 확인하므로
목록에 뜨지않는다.

kubectl get pod,deploy -n develop 
> 특정 namespace를 지정하여  리소스를 확인하기 


==config map/ secret ( 설정값을 포드에 전달하기 )
yaml파일과 설정값을 분리할수 있다.

configmap : 설정값중 일반적으로 변수 (환경변수), 파일의 내용등을 각 포드에 전달하고자 할때 사용
위와 같이 cofigmap 은 주로 변수 를 사전에 정의하고 이를 각 포드에 전달하여
포드에서 개발에 참여하는 모든 개발자들이 동일한 환경변수를 사용할 수 있도록 해 주는 것을 configmap 이라고 부른다. 

configmap 작성하기 
- kubectl create configmap(cm) "configmap name" 설정값

---configmap 실습---

kubectl create cm log-level-cmap --from-literal LOG_LEVEL=DEBUG

kubectl create cm start-k8s --from-literal k8s=kubernetes --from-literal container=docker

kubectl get cm

 kubectl describe cm log-level-cmap
-------------------------------------------
Data
====
LOG_LEVEL:
----
DEBUG
--------------------------------------------

kubectl describe cm start-k8s
-----------------------------------------
Data
====
container:
----
docker
k8s:
----
kubernetes
-------------------------------------------

kubectl get cm log-level-cmap -o yaml
kubectl get cm start-k8s -o yaml

-configmap을 이용한 pod배포하기 

 vi cmap1.yaml
-----------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: container-env-test
spec:
  containers:
  - name: env-ctn
    image: centos:7
    args: ["tail", "-f", "/dev/null"]
    envFrom:
    - configMapRef:
        name: log-level-cmap
    - configMapRef:
        name: start-k8s
-----------------------------------------


kubectl exec container-env-test env
> 
LOG_LEVEL=DEBUG
container=docker
k8s=kubernetes
가 들어있는지 확인

-2개이상의 key:value 쌍이 있는데이터에서 특정 key:value만을 적용시키는 경우

vi cmap2.yaml
----------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: container-env-test
spec:
  containers:
  - name: env-ctn
    image: centos:7
    args: ["tail", "-f", "/dev/null"]
    env:
    - name: KEY1
      valueFrom:
        configMapKeyRef:
          name: log-level-cmap
          key: LOG_LEVEL
    - name: KEY2
      valueFrom:
        configMapKeyRef:
          name: start-k8s
          key: k8s
-----------------------------------------------------

 kubectl apply -f cmap2.yaml

 kubectl exec container-env-test env | grep KEY
---------------------------------------------
KEY1=DEBUG
KEY2=kubernetes
---------------------------------------------

-특정 파일에 cmap적용하기 (camp을 파일에 마운트하기)

vi cmap3.yaml
-----------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: container-env-test
spec:
  containers:
  - name: env-ctn
    image: centos:7
    args: ["tail", "-f", "/dev/null"]
    volumeMounts:
    - name: cmap-volume
      mountPath: /etc/config

  volumes:
  - name: cmap-volume
    configMap:
      name: start-k8s
----------------------------------------------

kubectl apply -f cmap3.yaml

kubectl exec -it container-env-test -- ls -l /etc/config/
total 0
lrwxrwxrwx 1 root root 16 May 21 02:37 container -> ..data/container
lrwxrwxrwx 1 root root 10 May 21 02:37 k8s -> ..data/k8s


kubectl exec -it container-env-test -- cat /etc/config/k8s; echo ""
kubernetes

kubectl exec -it container-env-test -- cat /etc/config/container; echo ""
docker


---secret---
설정값중 보안을 요하는 패스워드와 관련된 설정을 포드에 전달하고자 할때 주로 사용

kubectl create secret generic my-password --from-literal password=test123

echo -e "password1"> pw1
echo -e "password2"> pw2

kubectl create secret generic pasword12 --from-file pw1 --from-file pw2
kubectl get secrets

kubectl get secrets my-password -o yaml
  password: dGVzdDEyMw==

echo dGVzdDEyMw== | base64 -d ; echo ""
test123

> 난독화 시킨값을 저장함

kubectl create secret generic my-pwd --from-literal password=test123 --dry-run -o yaml
apiVersion: v1
data:
  password: dGVzdDEyMw==
kind: Secret
metadata:
  creationTimestamp: null
  name: my-pwd


vi secret1.yaml
---------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-test
spec:
  containers:
  - name: secret-ctn
    image: centos:7
    args: ["tail", "-f", "/dev/null"]
    envFrom:
    - secretRef:
        name: my-password
----------------------------------------------------------
kubectl apply -f secret1.yaml

kubectl exec secret-env-test env
>시크릿을사용할때 yaml 파일에 base64로인코딩한값을입력했더라도
시크릿을 포드의환경변수나 볼륨으로가져오면
base64로 디코딩된 원래의값을 사용하게된다


https://kubernetes.io/ko/docs/tasks/configure-pod-container/pull-image-private-registry/

kubectl create secret docker-registry registry-key --docker-username=ciw0707 --docker-password=********

kubectl get secrets registry-key -o yaml
--------------------------------------------------------------------------------------
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOnsidXNlcm5hbWUiOiJjaXcwNzA3IiwicGFzc3dvcmQiOiJkaWFrMTM1MSEiLCJhdXRoIjoiWTJsM01EY3dOenBrYVdGck1UTTFNU0U9In19fQ==
kind: Secret
metadata:
  creationTimestamp: "2021-05-21T04:40:36Z"
  name: registry-key
  namespace: default
  resourceVersion: "80083"
  uid: 98a57af1-ac36-4db4-87cf-e9f3be2ddf6d
type: kubernetes.io/dockerconfigjson
---------------------------------------------------------------------------------------


cat ~/.docker/config.json
--------------------------------------------------------------------
{
        "auths": {
                "https://index.docker.io/v1/": {
                        "auth": "Y2l3MDcwNzpkaWFrMTM1MSE="
                }
        },
        "HttpHeaders": {
                "User-Agent": "Docker-Client/18.09.9 (linux)"
        }
------------------------------------------------------------------

echo "Y2l3MDcwNzpkaWFrMTM1MSE=" | base64 -d > ID 비밀번호 확인가능



===PV, PVC 
PV(Persistent Volume) /PVC(Persistent Volume Claim) 

이미지를 이용하여  컨테이너(pod)를 배포했을 경우 컨테이너 내부에 저장된 데이터, 데이터베이스는
컨테이너가 삭제될경우 동일하게 삭제되어 별도로 접근이 불가능
이를방지하기 위하여 사용하는 것을 퍼시스턴트 볼륨(영구볼륨) 이라고 한다


1. 호스트와의 마운트(NFS)를 사용한다  : 호스트의 특정디렉토리와 컨테이너의 특정디렉토리를 연결하기 
2.컨테이너간 연결 : 사용x 
3.별도의 도커볼륨을 컨테이너의 특정 디렉토리와 연결하는 방법(iSCSI) 

*iSCASI : IP를 이용하여 연결되는 SCSI 

 쿠버네티스는 특정노드에서만 데이터를 보관해 저장한다면
 포드가 다른 노드로 옮겨지거나 생성되면
 해당데이터를 사용할수 없음 

 그러므로 특정 노드에서만 포드를 실행해야하는 상황이 발생
이를 해결하기위한 방법이 바로 PV이며,
네트워크로 연결해 사용할 수 있는 PV  의 대표적인 예는 NFS, EBS, Ceph, GlusterFS 등이 있다.

쿠버네티스는 PV 를 사용하기 위한 자체 기능을 제공하고 있다. 

 NFS 서버 준비 -> 방화벽 해제 -> /etc/exports 에 아래 내용 추가
   /k8s     211.183.3.0/24(rw,sync,no_root_squash) 
*사전에 /k8s 디렉토리를 만들어 두어야 한다.

 -> nfs-server 실행(또는 재실행)


==볼륨마운트하기

1.로컬호스트와 연결하기

2.원격지의 nfs-server 이용하기 


vi deployvol.yaml
------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-first-nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      color: black
  template:
    metadata:
      name: my-first-nginx
      labels:
        color: black
    spec:
      containers:
      - name: my-first-nginx-ctn
        image: nginx:1.10
        ports:
        - containerPort: 80
          protocol: TCP
        volumeMounts:
        - name: nodepath
          mountPath: /etc/localvol
        - name: nfspath
          mountPath: /etc/nfsvol

      volumes:
      - name: nodepath
        hostPath:
          path: /tmp
      - name: nfspath
        nfs:
          path: /k8s
          server: 211.183.3.99
--------------------------------------------------



kubectl apply  -f deployvol.yaml 


-nfs server 에서   touch /k8s/PVtest


kubectl exec -it my-first-nginx-deploy-7dcc967476-tkkn6 -- sh




ls /etc/*vol
------------------------------------------------------------------------------------------
/etc/localvol:
total 52
drwxrwxrwt 2 root root 4096 May 18 06:57 VMwareDnD
drwx------ 2 root root 4096 May 21 00:28 ansible_command_payload_1nt3gkj0
drwx------ 2 root root 4096 May 21 05:32 ansible_command_payload_4f308bmf
drwx------ 2 root root 4096 May 21 05:40 ansible_command_payload_5fz1omi9
drwx------ 3 root root 4096 May 20 04:48 snap.gnome-calculator
drwx------ 3 root root 4096 May 20 04:48 snap.gnome-characters
drwx------ 3 root root 4096 May 20 04:48 snap.gnome-logs
drwx------ 3 root root 4096 May 20 04:48 snap.gnome-system-monitor
drwx------ 3 root root 4096 May 18 06:57 systemd-private-ec4887ffd7ea475291cf0a4434149590-ModemManager.service-vMYW7E
drwx------ 3 root root 4096 May 20 00:53 systemd-private-ec4887ffd7ea475291cf0a4434149590-spice-vdagentd.service-NBDpKq
drwx------ 3 root root 4096 May 18 06:57 systemd-private-ec4887ffd7ea475291cf0a4434149590-systemd-resolved.service-wUytl6
drwx------ 3 root root 4096 May 18 06:57 systemd-private-ec4887ffd7ea475291cf0a4434149590-systemd-timesyncd.service-fviBpz
drwx------ 2 root root 4096 May 18 06:57 vmware-root_614-2722697888

/etc/nfsvol:
total 0
-rw-r--r-- 1 root root 0 May 21 06:13 PVtest
----------------------------------------------------------------------------------------------

-pvc를 이용한 pv할당 

vi nfs-pv.yaml
--------------------------------------------------------------------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
 capacity:
   storage: 1Gi
 accessModes:
   - ReadWriteOnce
 persistentVolumeReclaimPolicy: Retain
 nfs:
   path: /pvpvc
   server: 211.183.3.99
   readOnly: false
---------------------------------------------------------------------------
kubectl get pv



vi nfs-pod-pvc.yaml 
-------------------------------------------------------------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-nfs-pvc
spec:
 storageClassName: ""
 accessModes:
   - ReadWriteOnce
 resources:
   requests:
     storage: 1Gi

---
apiVersion: v1
kind: Pod
metadata:
 name: nfs-mount-ctn
spec:
  containers:
  - name: nfs-mount-ctn
    image: centos:7
    args: ["tail", "-f", "/dev/null"]
    volumeMounts:
    - name: nfs-volume
      mountPath: /mnt

  volumes:
  - name: nfs-volume
    persistentVolumeClaim:
      claimName: my-nfs-pvc
----------------------------------------------------------------------------

kubectl apply -f nfs-pod-pvc.yaml

kubectl exec -it nfs-mount-ctn -- /bin/bash
df -h | grep /mnt

> NFS연결시  1Gi로 마운트 시킨다 하더라도 
NFS로 연결된 디렉토리의 실제 크기가 나타나게 된다.

그러므로 실제 1Gi로의 할당을 위해서는
애초에 파티션을 나누어 1Gi 디스크를 마운트하여 nfs로 공유하거나 
혹은  iSCSI 를 사용해야 한다.



=ansible 

master node에  ansible설치하기 

apt-add-repository ppa:ansible/ansible
apt-get update
apt-get install ansible -y
ansible --version

*쿠버네티스 자체에서는 Load Blalancer를 제공하지않음
metallb 오픈스택의 LBAAS 등의 오픈 소스프로젝트를 사용하거나
수동으로 HAProxy서버를 구축하여 사용할수 있음


=pod의 자원사용량 제한
컨테이너의 자원사용량 제한
-cpus 직관적으로 CPU의 개수를 직접지정 
ex) docker container run -d --cpus 0.5 nginx > cpu의 50%를 할당하여 사용 

---------- 실습 예제 -----------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edustack
spec:
  selector:
    matchLabels:
      color: black
  replicas: 1
  template:
    metadata:
      labels:
        color: black
    spec:
      containers:
      - name: edustack-web
        image: beomtaek78/edustack:1.0
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m

---

apiVersion: v1
kind: Service
metadata:
  name: edustack-np
spec:
  ports:
    - name: edustack-node-port
      port: 8080
      targetPort: 80
      nodePort: 30001
  selector:
    color: black
  type: NodePort
--------------------------------------------------------------------


오토 스케일링을 위한 메트릭-서버 설치
wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

136번 줄에 
- --kubelet-insecure-tls 추가 

kubectl apply -f components.yaml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created


kubectl top no --use-protocol-buffers
NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
node1    190m         4%     972Mi           52%
node2    192m         4%     983Mi           52%
node3    153m         3%     981Mi           52%
ubuntu   564m         14%    2390Mi          62%

>cpu , 메모리 사용량 확인 가능

*1000m : cpu 1개 ?

오토스케일러 작성하기 
-----------------------------------------------
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: test-hpa
spec:
  maxReplicas: 10
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: edustack
  targetCPUUtilizationPercentage: 20
-------------------------------------------------


kubectl autoscale deploy edustack --cpu-percent=20 --min=1 --max=15

 kubectl get hpa

 centos 서버를 하나 동작시켜서 yum -y install httpd-tools 설치

 ab -c 100 -n 300 -t 60 http://211.183.3.100:30001/

kubectl top no --use-protocol-buffers,  kubectl get hpa,pod



[과제 수행]
- 현재까지 작성된 모든 오브젝트 중 pod, rs, deploy, service, hpa 등등을 모두 삭제하라.
- kubernetes 클러스터 환경에서 아래의 조건을 만족하는 환경을 구축하라
 
- 이미지는 centos 내에 httpd  를 설치하고 80번 포트를 오픈한다. 단, 페이지의 내용은 임의대로한다.
 이미지는 1.0 버전과 2.0 버전 두개를 작성하고 index.html 파일의 내용만 달리한다. 두 이미지는 docker-hub에 미리 upload 해 둔다.
- deploy 를 이용하여 1.0 이미지의 pod 를 배포한다. 단, 초기는 1개만 배포한다.
- 이미지를 다운로드 할 때에는 Secret 을 적용하여 dockerhub 의 계정을 통해 이미지를 다운 받을 수 있어야 한다.
 (각 노드에서 docker login 하지 말것)
- Service 를 이용하여 외부에서 해당 포드로 접속이 가능해야 한다. 단, type 은 nodePort 를 사용하고
 이를 외부에 구성된 HAProxy 를 통해 접근할 수 있어야 한다.
- auto-scale 을 구성하여 외부 접속량에 따라 자동으로 스케일이 up->down 될 수 있어야 한다.

- 외부에서 트래픽을 전송하고 이를 처리하는 결과를 그래프로 나타내라
   이를 위해 외부에서 트래픽은 ab 를 이용하고 그래프는 gnuplot 을 이용한다.
   그래프는 시간에 따라 외부에서 전송된 트래픽을 처리한 결과가 나타나야 한다.    

- 그래프의 결과를 토대로 deploy 내의 컨테이너에 대한 resource 를 적절히 조정하고 다시 외부에서 ab 를 이용하여 
처리하는 시간을 줄이는 조정을 적용하라.
------------------------------------------------------------
추가) 프로메테우스+그라파나를 적용하여 그래프로 리소스에 대한 처리 내역등을 시각화 하라.
https://arisu1000.tistory.com/27857?category=787056

 위의 과제가 완성된 사람은 결과를 jpg 파일로 작성한 뒤 이를 upload 하세요!!